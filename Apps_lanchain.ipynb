{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.Completions object at 0x000001D9B0177C50> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001D9B0199A60> root_client=<openai.OpenAI object at 0x000001D9AFE67E60> root_async_client=<openai.AsyncOpenAI object at 0x000001D9B0177CB0> model_name='gpt-4o' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model = \"gpt-4o\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = llm.invoke(\"what is generative AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Generative AI refers to a category of artificial intelligence systems designed to generate new content, such as text, images, music, or even video, by learning from existing data. Unlike traditional AI, which typically focuses on recognizing patterns or making predictions, generative AI creates novel outputs that are similar to the data it was trained on.\\n\\nSome key characteristics and components of generative AI include:\\n\\n1. **Deep Learning**: Generative AI often relies on deep learning techniques, particularly neural networks, to model complex patterns in data.\\n\\n2. **Models and Architectures**: Common models used in generative AI include Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Transformer-based models like GPT (Generative Pre-trained Transformer).\\n\\n3. **Applications**: Generative AI is used in a wide range of applications, such as creating realistic images (e.g., deepfakes), generating human-like text (e.g., chatbots), composing music, designing products, and more.\\n\\n4. **Training**: These systems are trained on large datasets and learn to understand the underlying structure of the data, enabling them to generate new content that follows similar patterns.\\n\\n5. **Creativity and Innovation**: Generative AI can be a powerful tool for creativity and innovation, assisting artists, designers, and other professionals in exploring new ideas and concepts.\\n\\n6. **Ethical Considerations**: The ability of generative AI to create realistic and convincing content raises ethical concerns, including issues related to misinformation, copyright infringement, and the potential misuse of synthesized media.\\n\\nOverall, generative AI represents an exciting and rapidly evolving field with significant potential to transform various industries by enhancing creativity and automating content creation.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 347, 'prompt_tokens': 13, 'total_tokens': 360, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a79d8dac1f', 'finish_reason': 'stop', 'logprobs': None} id='run-0c0bd634-fc64-4f4c-bd14-1e08ca0eda4a-0' usage_metadata={'input_tokens': 13, 'output_tokens': 347, 'total_tokens': 360, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\" , \"You are an Expert AI Engineer . Provider me answer based on the question\") , \n",
    "        (\"user\" , \"{input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Langsmith is a tool designed to enhance the development and debugging process of applications that utilize large language models (LLMs). It provides developers with the ability to track, evaluate, and improve the performance of their LLM applications more efficiently. Through Langsmith, developers can maintain better control and understanding of how their models are interacting with data, identify any issues or areas for improvement, and iteratively refine their applications. This tool is particularly useful in the context of complex LLM pipelines where monitoring and optimization are critical to ensure the desired outcomes are achieved. As of my last update, Langsmith is part of the growing ecosystem of tools aimed at supporting developers in leveraging AI technologies effectively.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 135, 'prompt_tokens': 32, 'total_tokens': 167, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_9faba9f038', 'finish_reason': 'stop', 'logprobs': None} id='run-f76721ea-09b3-4851-a04e-62b65428ec36-0' usage_metadata={'input_tokens': 32, 'output_tokens': 135, 'total_tokens': 167, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"input\" : \"Can you tell me about langsmith\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The \"Attention is All You Need\" model, commonly referred to as the Transformer model, revolutionized the field of natural language processing (NLP) and is considered simpler in its architecture compared to previous models like RNNs and LSTMs. The key innovation in this model is the attention mechanism, which allows the model to weigh the importance of different words in a sentence when making predictions. Here\\'s a simplified explanation of the attention model:\\n\\n1. **Attention Mechanism**: \\n   - The attention mechanism enables the model to focus on specific parts of the input sequence when generating each part of the output sequence. It computes a set of attention scores that measure how relevant each word in the input sequence is to a particular word in the output sequence.\\n   - This is achieved through the use of three matrices â€“ Query (Q), Key (K), and Value (V). The attention scores are calculated as a dot product of Q and K, followed by a softmax operation to normalize them into probabilities. These scores are then used to weigh the values (V), generating a context vector that represents the input sequence\\'s information relevant to a specific position in the output.\\n\\n2. **Transformer Architecture**: \\n   - The Transformer model consists of an encoder and a decoder, both built using stacks of identical layers.\\n   - **Encoder**: Each encoder layer has two main components: a multi-head self-attention mechanism and a position-wise feedforward neural network. The self-attention mechanism allows each word in the input to attend to every other word, capturing contextual relationships.\\n   - **Decoder**: Similar to the encoder, but with an additional cross-attention mechanism that allows it to attend to the encoder\\'s output. This helps in generating the output sequence based on the encoded input.\\n\\n3. **Multi-Head Attention**: \\n   - Instead of calculating a single set of attention scores, multi-head attention projects the Q, K, and V matrices into multiple subspaces, allowing the model to attend to different parts of the sequence simultaneously and capture diverse contextual cues.\\n\\n4. **Positional Encoding**: \\n   - Since the Transformer model does not inherently account for the order of words due to the absence of recurrence, positional encoding is added to the input embeddings to provide information about the position of words in the sequence.\\n\\n5. **Parallelization and Efficiency**: \\n   - The attention mechanism allows for much greater parallelization compared to RNNs, as it does not rely on sequential processing of the input data. This makes Transformers highly efficient and scalable, capable of training on large datasets much faster.\\n\\nOverall, the attention model simplifies the process of capturing dependencies between words in a sequence, resulting in more accurate and efficient models for tasks like translation, summarization, and language modeling.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 558, 'prompt_tokens': 35, 'total_tokens': 593, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_9faba9f038', 'finish_reason': 'stop', 'logprobs': None} id='run-15755b5b-9a0b-4ee1-8eb3-344afb1adb94-0' usage_metadata={'input_tokens': 35, 'output_tokens': 558, 'total_tokens': 593, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"input\" : \"explain about the attention model is simaple\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The attention model is a mechanism within neural networks, particularly useful in natural language processing (NLP) and other sequential data tasks, which allows the model to focus on specific parts of the input data when making predictions. Here's a simplified explanation:\n",
      "\n",
      "1. **Problem with Traditional Models**: In traditional sequence-to-sequence models, like those using recurrent neural networks (RNNs), the entire input sequence is compressed into a fixed-size context vector. This can be problematic, especially for long sequences, as important information might get lost, leading to less accurate predictions.\n",
      "\n",
      "2. **Idea of Attention**: Attention addresses this issue by allowing the model to dynamically focus on different parts of the input sequence when making a prediction. Instead of relying on a single context vector, the model can look at the entire input sequence and weigh the importance of each part when producing each element of the output sequence.\n",
      "\n",
      "3. **How Attention Works**:\n",
      "   - **Score Calculation**: For each word in the input sequence, a score is calculated that represents how important that word is for predicting the current word in the output sequence.\n",
      "   - **Weights**: These scores are then normalized using a softmax function to produce a set of weights. These weights determine how much attention each input word receives.\n",
      "   - **Context Vector**: A context vector is created as a weighted sum of the input words' representations. This context vector is then used to make predictions for the output sequence.\n",
      "\n",
      "4. **Applications**: The attention mechanism is a core component of Transformer models, like BERT and GPT, which are state-of-the-art in many NLP tasks. It is also used in image processing tasks to focus on particular areas of an image.\n",
      "\n",
      "5. **Benefits**:\n",
      "   - **Flexibility**: Attention allows models to handle variable-length input sequences more effectively.\n",
      "   - **Interpretability**: It provides insights into which parts of the input the model is focusing on, making it easier to understand and interpret model decisions.\n",
      "\n",
      "By allowing models to consider the entire input sequence and dynamically focus on different parts, attention models have significantly improved the performance and versatility of neural networks in handling complex tasks.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | llm | output_parser\n",
    "response = chain.invoke({\"input\": \"explain about the attention model is easy way\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
